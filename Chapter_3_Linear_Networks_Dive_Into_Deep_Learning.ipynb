{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 3 Linear Networks Dive Into Deep Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJHnW3g70AXvMxOQEqad9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahrie/PembelajaranMesin-Week-9/blob/main/Chapter_3_Linear_Networks_Dive_Into_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Library"
      ],
      "metadata": {
        "id": "BFuu5vQye_yx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwYO95GTc4w9"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from d2l import torch as d2l\n",
        "\n",
        "from IPython import display\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "\n",
        "d2l.use_svg_display()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Vectorization For Speed"
      ],
      "metadata": {
        "id": "79QDD_4ReVPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10000\n",
        "a = torch.ones(n)\n",
        "b = torch.ones(n)"
      ],
      "metadata": {
        "id": "7U3utwzfeTVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:  \n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()"
      ],
      "metadata": {
        "id": "Lpg8R1yOe0pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = torch.zeros(n)\n",
        "timer = Timer()\n",
        "for i in range(n):\n",
        "    c[i] = a[i] + b[i]\n",
        "f'{timer.stop():.5f} sec'\n"
      ],
      "metadata": {
        "id": "TVbjTevde2AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timer.start()\n",
        "d = a + b\n",
        "f'{timer.stop():.5f} sec'"
      ],
      "metadata": {
        "id": "QX4Ja-FOe4iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Normal Distribution and Squared Loss"
      ],
      "metadata": {
        "id": "RRQcnwx3e6Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normal(x, mu, sigma):\n",
        "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
        "    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)"
      ],
      "metadata": {
        "id": "cE9shnyjfLpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use numpy again for visualization\n",
        "x = np.arange(-7, 7, 0.01)\n",
        "\n",
        "# Mean and standard deviation pairs\n",
        "params = [(0, 1), (0, 2), (3, 1)]\n",
        "d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',\n",
        "         ylabel='p(x)', figsize=(4.5, 2.5),\n",
        "         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])"
      ],
      "metadata": {
        "id": "EJzK_CSffM7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Scratch Generating The Dataset"
      ],
      "metadata": {
        "id": "O1C_csWFfPca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synthetic_data(w, b, num_examples):\n",
        "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
        "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
        "    y = torch.matmul(X, w) + b\n",
        "    y += torch.normal(0, 0.01, y.shape)\n",
        "    return X, y.reshape((-1, 1))"
      ],
      "metadata": {
        "id": "POZlQFIMfP_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_w = torch.tensor([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)"
      ],
      "metadata": {
        "id": "SXJ0i35ZfRLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('features:', features[0], '\\nlabel:', labels[0])"
      ],
      "metadata": {
        "id": "0opgDy0ffSiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.set_figsize()\n",
        "# The semicolon is for displaying the plot only\n",
        "d2l.plt.scatter(features[:, (1)].detach().numpy(),\n",
        "                labels.detach().numpy(), 1);"
      ],
      "metadata": {
        "id": "sLmYLusHfUB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading The Dataset"
      ],
      "metadata": {
        "id": "6nMAkUXkfWi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # The examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        batch_indices = torch.tensor(indices[i:min(i +\n",
        "                                                   batch_size, num_examples)])\n",
        "        yield features[batch_indices], labels[batch_indices]"
      ],
      "metadata": {
        "id": "imEQyFNTfc_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "for X, y in data_iter(batch_size, features, labels):\n",
        "    print(X, '\\n', y)\n",
        "    break"
      ],
      "metadata": {
        "id": "E2rkPoEDfgFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Model Parameter"
      ],
      "metadata": {
        "id": "P20-GpzDfgpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "uu50OyXOfi6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining The Model"
      ],
      "metadata": {
        "id": "2TUbVR3bfm2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linreg(X, w, b):  \n",
        "    \"\"\"The linear regression model.\"\"\"\n",
        "    return torch.matmul(X, w) + b"
      ],
      "metadata": {
        "id": "vu714jclfkn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining The Loss Function"
      ],
      "metadata": {
        "id": "flAVMp7Rfo3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def squared_loss(y_hat, y): \n",
        "    \"\"\"Squared loss.\"\"\"\n",
        "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
      ],
      "metadata": {
        "id": "PU7ll1WVfqWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deffining The Optimization Algorithm"
      ],
      "metadata": {
        "id": "zKO5ai4Zfr-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(params, lr, batch_size):  \n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad / batch_size\n",
        "            param.grad.zero_()"
      ],
      "metadata": {
        "id": "_R7D7PXHftfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "AACBaXL3fu56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.03\n",
        "num_epochs = 3\n",
        "net = linreg\n",
        "loss = squared_loss"
      ],
      "metadata": {
        "id": "IRreNHxnfwcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter(batch_size, features, labels):\n",
        "        l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`\n",
        "        # Compute gradient on `l` with respect to [`w`, `b`]\n",
        "        l.sum().backward()\n",
        "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
        "    with torch.no_grad():\n",
        "        train_l = loss(net(features, w, b), labels)\n",
        "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
      ],
      "metadata": {
        "id": "maNjrqXufx4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')\n",
        "print(f'error in estimating b: {true_b - b}')"
      ],
      "metadata": {
        "id": "EWMAfMvffzKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Image Classification Dataset Reading Dataset"
      ],
      "metadata": {
        "id": "E4Cw_psVf1Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `ToTensor` converts the image data from PIL type to 32-bit floating point\n",
        "# tensors. It divides all numbers by 255 so that all pixel values are between\n",
        "# 0 and 1\n",
        "trans = transforms.ToTensor()\n",
        "mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\", train=True,\n",
        "                                                transform=trans,\n",
        "                                                download=True)\n",
        "mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\", train=False,\n",
        "                                               transform=trans, download=True)"
      ],
      "metadata": {
        "id": "ErXFlI23f3a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mnist_train), len(mnist_test)"
      ],
      "metadata": {
        "id": "sxlMH6Rqf6D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train[0][0].shape"
      ],
      "metadata": {
        "id": "SEJzmCfEf7ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fashion_mnist_labels(labels): \n",
        "    \"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\n",
        "    text_labels = [\n",
        "        't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',\n",
        "        'sneaker', 'bag', 'ankle boot']\n",
        "    return [text_labels[int(i)] for i in labels]"
      ],
      "metadata": {
        "id": "J3n9k0gef8vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
        "    \"\"\"Plot a list of images.\"\"\"\n",
        "    figsize = (num_cols * scale, num_rows * scale)\n",
        "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
        "        if torch.is_tensor(img):\n",
        "            # Tensor Image\n",
        "            ax.imshow(img.numpy())\n",
        "        else:\n",
        "            # PIL Image\n",
        "            ax.imshow(img)\n",
        "        ax.axes.get_xaxis().set_visible(False)\n",
        "        ax.axes.get_yaxis().set_visible(False)\n",
        "        if titles:\n",
        "            ax.set_title(titles[i])\n",
        "    return axes"
      ],
      "metadata": {
        "id": "62RER1gsf-NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\n",
        "show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));"
      ],
      "metadata": {
        "id": "YEHJn8kYf_r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "def get_dataloader_workers(): \n",
        "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
        "    return 4\n",
        "\n",
        "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                             num_workers=get_dataloader_workers())"
      ],
      "metadata": {
        "id": "pQj2EgEYgA_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timer = d2l.Timer()\n",
        "for X, y in train_iter:\n",
        "    continue\n",
        "f'{timer.stop():.2f} sec'"
      ],
      "metadata": {
        "id": "xsDlwsbogCPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_fashion_mnist(batch_size, resize=None): \n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
        "    trans = [transforms.ToTensor()]\n",
        "    if resize:\n",
        "        trans.insert(0, transforms.Resize(resize))\n",
        "    trans = transforms.Compose(trans)\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
        "                                                    train=True,\n",
        "                                                    transform=trans,\n",
        "                                                    download=True)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
        "                                                   train=False,\n",
        "                                                   transform=trans,\n",
        "                                                   download=True)\n",
        "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                            num_workers=get_dataloader_workers()),\n",
        "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
        "                            num_workers=get_dataloader_workers()))"
      ],
      "metadata": {
        "id": "Nq47pTH-gDqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\n",
        "for X, y in train_iter:\n",
        "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
        "    break"
      ],
      "metadata": {
        "id": "Dxh_0w6RgGUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ],
      "metadata": {
        "id": "4YtJ5qH-gIEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "\n",
        "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
        "b = torch.zeros(num_outputs, requires_grad=True)"
      ],
      "metadata": {
        "id": "wtBd8EA8gKcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "X.sum(0, keepdim=True), X.sum(1, keepdim=True)"
      ],
      "metadata": {
        "id": "cm6eLBGqgL23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(X):\n",
        "    X_exp = torch.exp(X)\n",
        "    partition = X_exp.sum(1, keepdim=True)\n",
        "    return X_exp / partition  # The broadcasting mechanism is applied here"
      ],
      "metadata": {
        "id": "qvgfIACGgNhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.normal(0, 1, (2, 5))\n",
        "X_prob = softmax(X)\n",
        "X_prob, X_prob.sum(1)"
      ],
      "metadata": {
        "id": "3qWMMJ0lgO7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net(X):\n",
        "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
      ],
      "metadata": {
        "id": "jm6gSGAZgQMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([0, 2])\n",
        "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
        "y_hat[[0, 1], y]"
      ],
      "metadata": {
        "id": "O3-wEzTWgRdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y_hat, y):\n",
        "    return -torch.log(y_hat[range(len(y_hat)), y])\n",
        "\n",
        "cross_entropy(y_hat, y)"
      ],
      "metadata": {
        "id": "pplzcZc1gSpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_hat, y): \n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = y_hat.argmax(axis=1)\n",
        "    cmp = y_hat.type(y.dtype) == y\n",
        "    return float(cmp.type(y.dtype).sum())"
      ],
      "metadata": {
        "id": "En-FaOcOgUAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(y_hat, y) / len(y)"
      ],
      "metadata": {
        "id": "MG6ZUuZUgVg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(net, data_iter): \n",
        "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "    if isinstance(net, torch.nn.Module):\n",
        "        net.eval()  # Set the model to evaluation mode\n",
        "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            metric.add(accuracy(net(X), y), y.numel())\n",
        "    return metric[0] / metric[1]"
      ],
      "metadata": {
        "id": "sMdDod7LgWKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accumulator:  \n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "BPo_xiJEgXuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_accuracy(net, test_iter)"
      ],
      "metadata": {
        "id": "pIFPaaABgaTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ch3(net, train_iter, loss, updater):  \n",
        "    \"\"\"The training loop defined in Chapter 3.\"\"\"\n",
        "    # Set the model to training mode\n",
        "    if isinstance(net, torch.nn.Module):\n",
        "        net.train()\n",
        "    # Sum of training loss, sum of training accuracy, no. of examples\n",
        "    metric = Accumulator(3)\n",
        "    for X, y in train_iter:\n",
        "        # Compute gradients and update parameters\n",
        "        y_hat = net(X)\n",
        "        l = loss(y_hat, y)\n",
        "        if isinstance(updater, torch.optim.Optimizer):\n",
        "            # Using PyTorch in-built optimizer & loss criterion\n",
        "            updater.zero_grad()\n",
        "            l.backward()\n",
        "            updater.step()\n",
        "            metric.add(float(l) * len(y), accuracy(y_hat, y), y.numel())\n",
        "        else:\n",
        "            # Using custom built optimizer & loss criterion\n",
        "            l.sum().backward()\n",
        "            updater(X.shape[0])\n",
        "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
        "    # Return training loss and training accuracy\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]"
      ],
      "metadata": {
        "id": "42QrVz94gbsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Animator:  \n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        d2l.use_svg_display()\n",
        "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [self.axes,]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: d2l.set_axes(self.axes[\n",
        "            0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)"
      ],
      "metadata": {
        "id": "6x_vt90ygdag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
        "    \"\"\"Train a model (defined in Chapter 3).\"\"\"\n",
        "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
        "                        legend=['train loss', 'train acc', 'test acc'])\n",
        "    for epoch in range(num_epochs):\n",
        "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
        "        test_acc = evaluate_accuracy(net, test_iter)\n",
        "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
        "    train_loss, train_acc = train_metrics\n",
        "    assert train_loss < 0.5, train_loss\n",
        "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
        "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
      ],
      "metadata": {
        "id": "SEdxTJMKge2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "\n",
        "def updater(batch_size):\n",
        "    return d2l.sgd([W, b], lr, batch_size)"
      ],
      "metadata": {
        "id": "C0Qy4CNIggXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
      ],
      "metadata": {
        "id": "h0uIHQpVghtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "myDo_ZczgjFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ch3(net, test_iter, n=6):  \n",
        "    \"\"\"Predict labels (defined in Chapter 3).\"\"\"\n",
        "    for X, y in test_iter:\n",
        "        break\n",
        "    trues = d2l.get_fashion_mnist_labels(y)\n",
        "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
        "    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
        "    d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])\n",
        "\n",
        "predict_ch3(net, test_iter)"
      ],
      "metadata": {
        "id": "AFFPKf--gkb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ],
      "metadata": {
        "id": "-w0jFicPgmFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch does not implicitly reshape the inputs. Thus we define the flatten\n",
        "# layer to reshape the inputs before the linear layer in our network\n",
        "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "net.apply(init_weights);"
      ],
      "metadata": {
        "id": "VpqVrVdUgnrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "SAmv3RxVgpEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = torch.optim.SGD(net.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "kOszJXMogqaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
      ],
      "metadata": {
        "id": "-JU5BBilgrvg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}