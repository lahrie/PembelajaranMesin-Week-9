{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4 Multilayer Perceptrons.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKv4N/WkC5c8LrBWGuVmW+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahrie/PembelajaranMesin-Week-9/blob/main/Chapter_4_Multilayer_Perceptrons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMNeEzgAg65B"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from d2l import torch as d2l\n",
        "from torch import nn\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
        "y = torch.relu(x)\n",
        "d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))"
      ],
      "metadata": {
        "id": "zUsBwG42hF3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward(torch.ones_like(x), retain_graph=True)\n",
        "d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))"
      ],
      "metadata": {
        "id": "ujwMFDe6hHp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.sigmoid(x)\n",
        "d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))"
      ],
      "metadata": {
        "id": "hGws0ksrhIxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear out previous gradients\n",
        "x.grad.data.zero_()\n",
        "y.backward(torch.ones_like(x), retain_graph=True)\n",
        "d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))"
      ],
      "metadata": {
        "id": "U6KtG17GhK27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tanh(x)\n",
        "d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))"
      ],
      "metadata": {
        "id": "988t5efGhNCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear out previous gradients.\n",
        "x.grad.data.zero_()\n",
        "y.backward(torch.ones_like(x), retain_graph=True)\n",
        "d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))"
      ],
      "metadata": {
        "id": "hl02fyTxhNy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ],
      "metadata": {
        "id": "eeqKbeGuhRgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
        "\n",
        "W1 = nn.Parameter(\n",
        "    torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)\n",
        "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
        "W2 = nn.Parameter(\n",
        "    torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)\n",
        "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n",
        "\n",
        "params = [W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "fUK5ZlJzhTj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(X):\n",
        "    a = torch.zeros_like(X)\n",
        "    return torch.max(X, a)"
      ],
      "metadata": {
        "id": "tPRNH8KOhVzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net(X):\n",
        "    X = X.reshape((-1, num_inputs))\n",
        "    H = relu(X @ W1 + b1)  # Here '@' stands for matrix multiplication\n",
        "    return (H @ W2 + b2)"
      ],
      "metadata": {
        "id": "ikNF48KDhWaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "OpQ1ncpwhX-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 10, 0.1\n",
        "updater = torch.optim.SGD(params, lr=lr)\n",
        "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
      ],
      "metadata": {
        "id": "ObV5xCVUhZTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.predict_ch3(net, test_iter)"
      ],
      "metadata": {
        "id": "ZEUAxsNchbuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),\n",
        "                    nn.Linear(256, 10))\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "net.apply(init_weights);"
      ],
      "metadata": {
        "id": "JVEAG8eghdOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),\n",
        "                    nn.Linear(256, 10))\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "net.apply(init_weights);"
      ],
      "metadata": {
        "id": "RIDynCckhesv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, lr, num_epochs = 256, 0.1, 10\n",
        "loss = nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "kKtf3iYThgms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
        "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
      ],
      "metadata": {
        "id": "KkKjwgV4hh-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_degree = 20  # Maximum degree of the polynomial\n",
        "n_train, n_test = 100, 100  # Training and test dataset sizes\n",
        "true_w = np.zeros(max_degree)  # Allocate lots of empty space\n",
        "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
        "\n",
        "features = np.random.normal(size=(n_train + n_test, 1))\n",
        "np.random.shuffle(features)\n",
        "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
        "for i in range(max_degree):\n",
        "    poly_features[:, i] /= math.gamma(i + 1)  # `gamma(n)` = (n-1)!\n",
        "# Shape of `labels`: (`n_train` + `n_test`,)\n",
        "labels = np.dot(poly_features, true_w)\n",
        "labels += np.random.normal(scale=0.1, size=labels.shape)"
      ],
      "metadata": {
        "id": "OPl1p9Vghjdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert from NumPy ndarrays to tensors\n",
        "true_w, features, poly_features, labels = [\n",
        "    torch.tensor(x, dtype=torch.float32)\n",
        "    for x in [true_w, features, poly_features, labels]]"
      ],
      "metadata": {
        "id": "VG22VmkLhl7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[:2], poly_features[:2, :], labels[:2]"
      ],
      "metadata": {
        "id": "eWsRPWDahnNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loss(net, data_iter, loss): \n",
        "    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
        "    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples\n",
        "    for X, y in data_iter:\n",
        "        out = net(X)\n",
        "        y = y.reshape(out.shape)\n",
        "        l = loss(out, y)\n",
        "        metric.add(l.sum(), l.numel())\n",
        "    return metric[0] / metric[1]"
      ],
      "metadata": {
        "id": "VHY0SQEshobk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_features, test_features, train_labels, test_labels,\n",
        "          num_epochs=400):\n",
        "    loss = nn.MSELoss()\n",
        "    input_shape = train_features.shape[-1]\n",
        "    # Switch off the bias since we already catered for it in the polynomial\n",
        "    # features\n",
        "    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
        "    batch_size = min(10, train_labels.shape[0])\n",
        "    train_iter = d2l.load_array((train_features, train_labels.reshape(-1, 1)),\n",
        "                                batch_size)\n",
        "    test_iter = d2l.load_array((test_features, test_labels.reshape(-1, 1)),\n",
        "                               batch_size, is_train=False)\n",
        "    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',\n",
        "                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\n",
        "                            legend=['train', 'test'])\n",
        "    for epoch in range(num_epochs):\n",
        "        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n",
        "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
        "            animator.add(epoch + 1, (evaluate_loss(\n",
        "                net, train_iter, loss), evaluate_loss(net, test_iter, loss)))\n",
        "    print('weight:', net[0].weight.data.numpy())"
      ],
      "metadata": {
        "id": "xUb8QpSJhpzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the\n",
        "# polynomial features\n",
        "train(poly_features[:n_train, :4], poly_features[n_train:, :4],\n",
        "      labels[:n_train], labels[n_train:])"
      ],
      "metadata": {
        "id": "lWdd9H4phqVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick the first two dimensions, i.e., 1, x, from the polynomial features\n",
        "train(poly_features[:n_train, :2], poly_features[n_train:, :2],\n",
        "      labels[:n_train], labels[n_train:])"
      ],
      "metadata": {
        "id": "24tDj7bNhrle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick all the dimensions from the polynomial features\n",
        "train(poly_features[:n_train, :], poly_features[n_train:, :],\n",
        "      labels[:n_train], labels[n_train:], num_epochs=1500)"
      ],
      "metadata": {
        "id": "rzqSQxdahtJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick all the dimensions from the polynomial features\n",
        "train(poly_features[:n_train, :], poly_features[n_train:, :],\n",
        "      labels[:n_train], labels[n_train:], num_epochs=1500)"
      ],
      "metadata": {
        "id": "73bHeUeVhvkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
        "true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n",
        "train_data = d2l.synthetic_data(true_w, true_b, n_train)\n",
        "train_iter = d2l.load_array(train_data, batch_size)\n",
        "test_data = d2l.synthetic_data(true_w, true_b, n_test)\n",
        "test_iter = d2l.load_array(test_data, batch_size, is_train=False)"
      ],
      "metadata": {
        "id": "PaXTMISfhxl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params():\n",
        "    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n",
        "    b = torch.zeros(1, requires_grad=True)\n",
        "    return [w, b]"
      ],
      "metadata": {
        "id": "mVJSSn-Ehy3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_penalty(w):\n",
        "    return torch.sum(w.pow(2)) / 2"
      ],
      "metadata": {
        "id": "8pU9AXR8h0Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(lambd):\n",
        "    w, b = init_params()\n",
        "    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
        "    num_epochs, lr = 100, 0.003\n",
        "    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n",
        "                            xlim=[5, num_epochs], legend=['train', 'test'])\n",
        "    for epoch in range(num_epochs):\n",
        "        for X, y in train_iter:\n",
        "            # The L2 norm penalty term has been added, and broadcasting\n",
        "            # makes `l2_penalty(w)` a vector whose length is `batch_size`\n",
        "            l = loss(net(X), y) + lambd * l2_penalty(w)\n",
        "            l.sum().backward()\n",
        "            d2l.sgd([w, b], lr, batch_size)\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\n",
        "                                     d2l.evaluate_loss(net, test_iter, loss)))\n",
        "    print('L2 norm of w:', torch.norm(w).item())"
      ],
      "metadata": {
        "id": "PKoARjiHh13S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(lambd=0)"
      ],
      "metadata": {
        "id": "LcFevAHmh2jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(lambd=3)"
      ],
      "metadata": {
        "id": "53uuWFh9h3_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_concise(wd):\n",
        "    net = nn.Sequential(nn.Linear(num_inputs, 1))\n",
        "    for param in net.parameters():\n",
        "        param.data.normal_()\n",
        "    loss = nn.MSELoss()\n",
        "    num_epochs, lr = 100, 0.003\n",
        "    # The bias parameter has not decayed\n",
        "    trainer = torch.optim.SGD([{\n",
        "        \"params\": net[0].weight,\n",
        "        'weight_decay': wd}, {\n",
        "            \"params\": net[0].bias}], lr=lr)\n",
        "    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n",
        "                            xlim=[5, num_epochs], legend=['train', 'test'])\n",
        "    for epoch in range(num_epochs):\n",
        "        for X, y in train_iter:\n",
        "            trainer.zero_grad()\n",
        "            l = loss(net(X), y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\n",
        "                                     d2l.evaluate_loss(net, test_iter, loss)))\n",
        "    print('L2 norm of w:', net[0].weight.norm().item())"
      ],
      "metadata": {
        "id": "BQS_zvOch616"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_concise(0)"
      ],
      "metadata": {
        "id": "07rkxtP0h84t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_concise(3)"
      ],
      "metadata": {
        "id": "P3EgtWlFh-Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iwExD9SniBUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dropout_layer(X, dropout):\n",
        "    assert 0 <= dropout <= 1\n",
        "    # In this case, all elements are dropped out\n",
        "    if dropout == 1:\n",
        "        return torch.zeros_like(X)\n",
        "    # In this case, all elements are kept\n",
        "    if dropout == 0:\n",
        "        return X\n",
        "    mask = (torch.rand(X.shape) > dropout).float()\n",
        "    return mask * X / (1.0 - dropout)"
      ],
      "metadata": {
        "id": "hfG1KLtQwfPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(16, dtype=torch.float32).reshape((2, 8))\n",
        "print(X)\n",
        "print(dropout_layer(X, 0.))\n",
        "print(dropout_layer(X, 0.5))\n",
        "print(dropout_layer(X, 1.))"
      ],
      "metadata": {
        "id": "eIB6cJpyiDGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256"
      ],
      "metadata": {
        "id": "CJHOUOpHiFrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout1, dropout2 = 0.2, 0.5\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n",
        "                 is_training=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.num_inputs = num_inputs\n",
        "        self.training = is_training\n",
        "        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n",
        "        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n",
        "        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n",
        "        # Use dropout only when training the model\n",
        "        if self.training == True:\n",
        "            # Add a dropout layer after the first fully connected layer\n",
        "            H1 = dropout_layer(H1, dropout1)\n",
        "        H2 = self.relu(self.lin2(H1))\n",
        "        if self.training == True:\n",
        "            # Add a dropout layer after the second fully connected layer\n",
        "            H2 = dropout_layer(H2, dropout2)\n",
        "        out = self.lin3(H2)\n",
        "        return out\n",
        "\n",
        "net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)"
      ],
      "metadata": {
        "id": "JVeul5auiGzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr, batch_size = 10, 0.5, 256\n",
        "loss = nn.CrossEntropyLoss()\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
      ],
      "metadata": {
        "id": "XNY30vW_iIGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),\n",
        "    # Add a dropout layer after the first fully connected layer\n",
        "    nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(),\n",
        "    # Add a dropout layer after the second fully connected layer\n",
        "    nn.Dropout(dropout2), nn.Linear(256, 10))\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "net.apply(init_weights);"
      ],
      "metadata": {
        "id": "tLoQqmmGiJQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
      ],
      "metadata": {
        "id": "yM6pzxefiJwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
        "y = torch.sigmoid(x)\n",
        "y.backward(torch.ones_like(x))\n",
        "\n",
        "d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],\n",
        "         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))"
      ],
      "metadata": {
        "id": "WtyJ1f-viLHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M = torch.normal(0, 1, size=(4, 4))\n",
        "print('a single matrix \\n', M)\n",
        "for i in range(100):\n",
        "    M = torch.mm(M, torch.normal(0, 1, size=(4, 4)))\n",
        "\n",
        "print('after multiplying 100 matrices\\n', M)"
      ],
      "metadata": {
        "id": "mbgauPuGiNaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'"
      ],
      "metadata": {
        "id": "AHbxVHHQiO8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(name, cache_dir=os.path.join('..', 'data')): \n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname"
      ],
      "metadata": {
        "id": "I7o57hBdiQNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_extract(name, folder=None):\n",
        "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def download_all():  #\n",
        "    \"\"\"Download all files in the DATA_HUB.\"\"\"\n",
        "    for name in DATA_HUB:\n",
        "        download(name)"
      ],
      "metadata": {
        "id": "RDkj5wRPiRid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB['kaggle_house_train'] = (  #\n",
        "    DATA_URL + 'kaggle_house_pred_train.csv',\n",
        "    '585e9cc93e70b39160e7921475f9bcd7d31219ce')\n",
        "\n",
        "DATA_HUB['kaggle_house_test'] = (  #\n",
        "    DATA_URL + 'kaggle_house_pred_test.csv',\n",
        "    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')"
      ],
      "metadata": {
        "id": "DM_5rnLgiSpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(download('kaggle_house_train'))\n",
        "test_data = pd.read_csv(download('kaggle_house_test'))"
      ],
      "metadata": {
        "id": "W2CvbqjkiTxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "k2arTa2MiVAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])"
      ],
      "metadata": {
        "id": "PZZXgfl2iWAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"
      ],
      "metadata": {
        "id": "NgXswx7siXO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If test data were inaccessible, mean and standard deviation could be\n",
        "# calculated from training data\n",
        "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
        "all_features[numeric_features] = all_features[numeric_features].apply(\n",
        "    lambda x: (x - x.mean()) / (x.std()))\n",
        "# After standardizing the data all means vanish, hence we can set missing\n",
        "# values to 0\n",
        "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
      ],
      "metadata": {
        "id": "Mpm8U-b9iYjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = train_data.shape[0]\n",
        "train_features = torch.tensor(all_features[:n_train].values,\n",
        "                              dtype=torch.float32)\n",
        "test_features = torch.tensor(all_features[n_train:].values,\n",
        "                             dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_data.SalePrice.values.reshape(-1, 1),\n",
        "                            dtype=torch.float32)"
      ],
      "metadata": {
        "id": "NBb7_vFJiaQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.MSELoss()\n",
        "in_features = train_features.shape[1]\n",
        "\n",
        "def get_net():\n",
        "    net = nn.Sequential(nn.Linear(in_features, 1))\n",
        "    return net"
      ],
      "metadata": {
        "id": "EYYRXAzhig1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_rmse(net, features, labels):\n",
        "    # To further stabilize the value when the logarithm is taken, set the\n",
        "    # value less than 1 as 1\n",
        "    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n",
        "    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))\n",
        "    return rmse.item()"
      ],
      "metadata": {
        "id": "NbQSVOF_ii0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, train_features, train_labels, test_features, test_labels,\n",
        "          num_epochs, learning_rate, weight_decay, batch_size):\n",
        "    train_ls, test_ls = [], []\n",
        "    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n",
        "    # The Adam optimization algorithm is used here\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=weight_decay)\n",
        "    for epoch in range(num_epochs):\n",
        "        for X, y in train_iter:\n",
        "            optimizer.zero_grad()\n",
        "            l = loss(net(X), y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "        train_ls.append(log_rmse(net, train_features, train_labels))\n",
        "        if test_labels is not None:\n",
        "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
        "    return train_ls, test_ls"
      ],
      "metadata": {
        "id": "qKlNm8R7ikM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_fold_data(k, i, X, y):\n",
        "    assert k > 1\n",
        "    fold_size = X.shape[0] // k\n",
        "    X_train, y_train = None, None\n",
        "    for j in range(k):\n",
        "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
        "        X_part, y_part = X[idx, :], y[idx]\n",
        "        if j == i:\n",
        "            X_valid, y_valid = X_part, y_part\n",
        "        elif X_train is None:\n",
        "            X_train, y_train = X_part, y_part\n",
        "        else:\n",
        "            X_train = torch.cat([X_train, X_part], 0)\n",
        "            y_train = torch.cat([y_train, y_part], 0)\n",
        "    return X_train, y_train, X_valid, y_valid"
      ],
      "metadata": {
        "id": "LIu3F_l2imWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,\n",
        "           batch_size):\n",
        "    train_l_sum, valid_l_sum = 0, 0\n",
        "    for i in range(k):\n",
        "        data = get_k_fold_data(k, i, X_train, y_train)\n",
        "        net = get_net()\n",
        "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
        "                                   weight_decay, batch_size)\n",
        "        train_l_sum += train_ls[-1]\n",
        "        valid_l_sum += valid_ls[-1]\n",
        "        if i == 0:\n",
        "            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],\n",
        "                     xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],\n",
        "                     legend=['train', 'valid'], yscale='log')\n",
        "        print(f'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, '\n",
        "              f'valid log rmse {float(valid_ls[-1]):f}')\n",
        "    return train_l_sum / k, valid_l_sum / k"
      ],
      "metadata": {
        "id": "q_ygZzSSioml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n",
        "train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\n",
        "                          weight_decay, batch_size)\n",
        "print(f'{k}-fold validation: avg train log rmse: {float(train_l):f}, '\n",
        "      f'avg valid log rmse: {float(valid_l):f}')"
      ],
      "metadata": {
        "id": "epkLXf94iqnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_pred(train_features, test_feature, train_labels, test_data,\n",
        "                   num_epochs, lr, weight_decay, batch_size):\n",
        "    net = get_net()\n",
        "    train_ls, _ = train(net, train_features, train_labels, None, None,\n",
        "                        num_epochs, lr, weight_decay, batch_size)\n",
        "    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',\n",
        "             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')\n",
        "    print(f'train log rmse {float(train_ls[-1]):f}')\n",
        "    # Apply the network to the test set\n",
        "    preds = net(test_features).detach().numpy()\n",
        "    # Reformat it to export to Kaggle\n",
        "    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
        "    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n",
        "    submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "GoWFi9Qoisav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_pred(train_features, test_features, train_labels, test_data,\n",
        "               num_epochs, lr, weight_decay, batch_size)"
      ],
      "metadata": {
        "id": "m_Yc2JiJis7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}